{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and initialization of general parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pareto_fairness import compute_pareto_metrics\n",
    "from config.info import AGES, RACES, GENDERS, COMBS_BASELINE\n",
    "from visualization.subgroup_distribution import plot_dist\n",
    "from dataprocess.dataloader import load_data\n",
    "from dataprocess.dataclass import Data\n",
    "from config.get_args import get_args\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from lightning import seed_everything\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Auto reload part\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the desired data set\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "metrics_to_test = ['MMPF', 'MMPF_5', 'MMPF_10', 'MMPF_size', 'MMPF_adapted']\n",
    "n_seeds = 20\n",
    "\n",
    "def check_metrics(task, cancer, df : pd.DataFrame = None):\n",
    "    # Initialization\n",
    "    metrics = {'MMPF' : [],\n",
    "            'MMPF_size' : [],\n",
    "            'MMPF_5' : [], 'MMPF_10' : [],\n",
    "            'MMPF_adapted' : []}\n",
    "    \n",
    "    # Extract the results pkl files\n",
    "    preds_path = 'results/preds/run_3/custom_subgroups_0/Baseline'\n",
    "    results = pd.read_pickle(preds_path + f'/{task}/{cancer}/results.pkl')\n",
    "    results.drop_duplicates(subset = ['subj'], inplace = True, keep = 'first')\n",
    "\n",
    "    # Get multiple seeds\n",
    "    seeds = []\n",
    "    for idx in range(n_seeds):\n",
    "        seeds += [random.randint(0, 10000)]\n",
    "\n",
    "    # Loop on multiple seeds\n",
    "    for seed in seeds:\n",
    "        # Seeding and init\n",
    "        random.seed(seed)\n",
    "        test_size = int(len(results) * 0.2)\n",
    "\n",
    "        # Extract the subjects and get the 20% test set\n",
    "        subj = results.subj.to_list()\n",
    "        random.shuffle(subj)\n",
    "        test_subj = subj[:test_size]\n",
    "\n",
    "        # Get the results of the data set only\n",
    "        test_results = results[results.subj.isin(test_subj)]\n",
    "        test_metrics = compute_pareto_metrics(test_results, loss_fct, ['age_', 'race_', 'gender_'], all_only = True)\n",
    "\n",
    "        # Add the metrics\n",
    "        for m in metrics_to_test:\n",
    "            metrics[m] += [test_metrics[m]]\n",
    "            \n",
    "    # Get the sub data frame\n",
    "    metrics_df = pd.DataFrame(data = metrics)\n",
    "    metrics_df['task'] = task\n",
    "    metrics_df['cancer'] = cancer\n",
    "    \n",
    "    # Concet the dataframe\n",
    "    if df is None: return metrics_df\n",
    "    else: return pd.concat([df, metrics_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop on the combinations\n",
    "df = None\n",
    "variance = {'task' : [], 'cancer': [],\n",
    "            'MMPF' : [],\n",
    "            'MMPF_size' : [],\n",
    "            'MMPF_5' : [], 'MMPF_10' : [],\n",
    "            'MMPF_adapted' : []}\n",
    "for comb in COMBS_BASELINE:\n",
    "    # Track\n",
    "    print(comb)\n",
    "    \n",
    "    # Extract combinations\n",
    "    task = comb[0]\n",
    "    cancer = comb[1]\n",
    "    \n",
    "    # Get the metrics\n",
    "    df = check_metrics(task, cancer, df = df)\n",
    "    \n",
    "    # Compute the variance\n",
    "    variance['task'] += [task]\n",
    "    variance['cancer'] += [cancer]\n",
    "    for m in metrics_to_test:\n",
    "        variance[m] += [df[(df.task == task) & (df.cancer == cancer)][m].var()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "px.box(df, y = metrics_to_test, color = 'cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in metrics_to_test:\n",
    "    print(m, ' ', sum(variance[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = pd.DataFrame(variance)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = var_df[var_df.task == 'cancer_classification']\n",
    "fig = go.Figure(go.Bar(x = red.cancer, y = red['MMPF_10']))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(var_df, x = 'cancer', y = metrics_to_test, log_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create manual scenarios\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build the distribution of the data among the different subgroups\n",
    "# Initialization\n",
    "metrics_to_test = ['MMPF', 'MMPF_5', 'MMPF_10', 'MMPF_size', 'MMPF_adapted']\n",
    "attributes = ['att_1', 'att_2']\n",
    "cols = attributes + ['pred_raw', 'label', 'pred']\n",
    "data_df = pd.DataFrame(columns = cols)\n",
    "range_atts = [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]\n",
    "nb_patients_sg = [[1 , 16, 50 , 61 , 42, 23], \n",
    "                  [3 , 2 , 1  , 7  , 15, 14], \n",
    "                  [8 , 3 , 3  , 1  , 5 , 1], \n",
    "                  [37, 93, 136, 120, 67, 11], \n",
    "                  [2 , 12, 22 , 18 , 15, 1], \n",
    "                  [1 , 1 , 2  , 4  , 1 , 2]]\n",
    "\n",
    "# Loop on attributes\n",
    "for att_1 in range_atts[0]:\n",
    "    for att_2 in range_atts[1]:\n",
    "        nb = nb_patients_sg[att_1][att_2]\n",
    "        sub_df = pd.DataFrame({'att_1' : [att_1] * nb,\n",
    "                               'att_2' : [att_2] * nb,\n",
    "                               'pred_raw' : [None] * nb,\n",
    "                               'label' : list(np.random.binomial(1, 0.5, nb)),\n",
    "                               'pred' : [None] * nb})\n",
    "        data_df = pd.concat([data_df, sub_df])\n",
    "data_df = data_df.astype({'att_1' : 'int32',\n",
    "                            'att_2' : 'int32',\n",
    "                            'label' : 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a fair and unfair scenario\n",
    "def build_scenario(data_df : pd.DataFrame, seed : int):\n",
    "    # Initialization\n",
    "    seed_everything(seed)\n",
    "    df_f = data_df.copy()\n",
    "    df_u = data_df.copy()\n",
    "    acc_pos = 0.9\n",
    "    acc_neg = 0.4\n",
    "    \n",
    "    \n",
    "    # Fair scenario\n",
    "    # Create 6x6 fair preds ratio\n",
    "    fair_ratio = np.random.normal(acc_pos, 0.1, 36).reshape((6, 6))\n",
    "    fair_ratio[fair_ratio >= 1] = 0.99\n",
    "    for att_1 in range_atts[0]:\n",
    "        for att_2 in range_atts[1]:\n",
    "            \n",
    "            # conditon subdf\n",
    "            cond = (df_f.att_1 == att_1) & (df_f.att_2 == att_2)\n",
    "            sub_df = df_f[cond].copy()\n",
    "            n_sub = len(sub_df)\n",
    "            \n",
    "            # Build the preds\n",
    "            n_good = int(n_sub * fair_ratio[att_1][att_2] + 0.5)\n",
    "            sub_preds = np.ones(n_sub, int)\n",
    "            sub_preds[:n_good] = 0\n",
    "            random.shuffle(sub_preds)\n",
    "            sub_preds = abs(sub_preds - np.array(df_f[cond].label))\n",
    "            df_f.loc[cond, 'pred'] = sub_preds\n",
    "            \n",
    "            # Build the raw preds\n",
    "            sub_preds_raw = []\n",
    "            poss = np.random.uniform(0.5, 1, n_sub)\n",
    "            negs = np.random.uniform(0, 0.5, n_sub)\n",
    "            for i, p in enumerate(sub_preds):\n",
    "                pos = poss[i]\n",
    "                neg = negs[i]\n",
    "                raw = [0, 0]\n",
    "                raw[int(p)] = pos\n",
    "                raw[int(1-p)] = neg\n",
    "                sub_preds_raw += [raw]\n",
    "            df_f.loc[cond, 'pred_raw'] = pd.Series(sub_preds_raw)\n",
    "            \n",
    "            \n",
    "    # Unfair scenario\n",
    "    # Create 6x6 unfair preds ratio\n",
    "    unfair_ratio1 = np.random.normal(acc_pos, 0.1, 18)\n",
    "    unfair_ratio2 = np.random.normal(acc_neg, 0.1, 18)\n",
    "    unfair_ratio1[unfair_ratio1 >= 1] = 0.99\n",
    "    unfair_ratio2[unfair_ratio2 >= 1] = 0.99\n",
    "    unfair_ratio = list(np.concatenate((unfair_ratio1, unfair_ratio2)))\n",
    "    random.shuffle(unfair_ratio)\n",
    "    unfair_ratio = np.array(unfair_ratio).reshape((6,6))\n",
    "    for att_1 in range_atts[0]:\n",
    "        for att_2 in range_atts[1]:\n",
    "            \n",
    "            # conditon subdf\n",
    "            cond = (df_u.att_1 == att_1) & (df_u.att_2 == att_2)\n",
    "            sub_df = df_u[cond].copy()\n",
    "            n_sub = len(sub_df)\n",
    "            \n",
    "            # Build the preds\n",
    "            n_good = int(n_sub * unfair_ratio[att_1][att_2] + 0.5)\n",
    "            sub_preds = np.ones(n_sub, int)\n",
    "            sub_preds[:n_good] = 0\n",
    "            random.shuffle(sub_preds)\n",
    "            sub_preds = abs(sub_preds - np.array(df_f[cond].label))\n",
    "            df_u.loc[cond, 'pred'] = sub_preds\n",
    "            \n",
    "            # Build the raw preds\n",
    "            sub_preds_raw = []\n",
    "            poss = np.random.uniform(0.5, 1, n_sub)\n",
    "            negs = np.random.uniform(0, 0.5, n_sub)\n",
    "            for i, p in enumerate(sub_preds):\n",
    "                pos = poss[i]\n",
    "                neg = negs[i]\n",
    "                raw = [0, 0]\n",
    "                raw[int(p)] = pos\n",
    "                raw[int(1-p)] = neg\n",
    "                sub_preds_raw += [raw]\n",
    "            df_u.loc[cond, 'pred_raw'] = pd.Series(sub_preds_raw)\n",
    "            \n",
    "    # reset indices\n",
    "    df_f.reset_index(inplace = True, drop = True)\n",
    "    df_f.reset_index(inplace = True, drop = False, names = 'subj')\n",
    "    df_u.reset_index(inplace = True, drop = True)\n",
    "    df_u.reset_index(inplace = True, drop = False, names = 'subj')\n",
    "    \n",
    "    # Return fair and unfair\n",
    "    return df_f, df_u, fair_ratio, unfair_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scenario(df_f, df_u, seed):\n",
    "    # Initialization\n",
    "    seed_everything(seed)\n",
    "    success = {'MMPF' : [],\n",
    "            'MMPF_size' : [],\n",
    "            'MMPF_5' : [], 'MMPF_10' : [],\n",
    "            'MMPF_adapted' : []}\n",
    "    \n",
    "    # Extract the subjects and get the 20% test set\n",
    "    test_subj_list = []\n",
    "    \n",
    "    # Get the values of unique subgroups prensent in the data set\n",
    "    unique_subgroups = np.unique(df_f[attributes].values, axis = 0)\n",
    "\n",
    "    # Loop on all the unique subgroups in the references\n",
    "    for subgroup in unique_subgroups:\n",
    "        \n",
    "        # Extract the references that are in this subgroup only\n",
    "        cond = df_f[attributes[0]] == subgroup[0]\n",
    "        for idx, att in enumerate(attributes[1:]):\n",
    "            cond = cond & (df_f[att] == subgroup[idx+1])\n",
    "        subjects = list(df_f[cond].subj)\n",
    "        \n",
    "        # Shuffle and split it between train / validation / test sets\n",
    "        random.shuffle(subjects)\n",
    "        if len(subjects) == 1: pass\n",
    "        elif len(subjects) == 2: pass\n",
    "        elif len(subjects) == 3: test_subj_list += [subjects[2]]\n",
    "        elif len(subjects) == 4: test_subj_list += [subjects[3]]\n",
    "        else:\n",
    "            sub_n = int(0.2 * len(subjects))\n",
    "            test_subj_list += subjects[: sub_n]\n",
    "\n",
    "    # Get the results of the data set only\n",
    "    test_results_fair = df_f[df_f.subj.isin(test_subj_list)]\n",
    "    test_results_unfair = df_u[df_u.subj.isin(test_subj_list)]\n",
    "    test_metrics_f = compute_pareto_metrics(test_results_fair, loss_fct, attributes, all_only = True)\n",
    "    test_metrics_u = compute_pareto_metrics(test_results_unfair, loss_fct, attributes, all_only = True)\n",
    "\n",
    "    # Add the metrics\n",
    "    for m in metrics_to_test:\n",
    "        if test_metrics_f[m] < test_metrics_u[m]: success[m] = 1\n",
    "        else: success[m] = 0\n",
    "        \n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "nb_success = {'MMPF' : 0,\n",
    "            'MMPF_size' : 0,\n",
    "            'MMPF_5' : 0, 'MMPF_10' : 0,\n",
    "            'MMPF_adapted' : 0}\n",
    "n_scenario = 100\n",
    "n_splits = 10\n",
    "n = 0\n",
    "\n",
    "# Get multiple seeds\n",
    "seeds_scenario = []\n",
    "for idx in range(n_scenario):\n",
    "    seeds_scenario += [random.randint(0, 1e7)]\n",
    "\n",
    "# Loop on the number of scenarios\n",
    "for seed_sc in seeds_scenario:\n",
    "    \n",
    "    # Get scenario \n",
    "    df_f, df_u, _, _ = build_scenario(data_df, seed_sc)\n",
    "    \n",
    "    # Get multiple seeds\n",
    "    seeds_splits = []\n",
    "    for idx in range(n_splits):\n",
    "        seeds_splits += [random.randint(0, 1e7)]\n",
    "    \n",
    "    # Loop on the splits\n",
    "    for seed_sp in seeds_splits:\n",
    "        print(n)\n",
    "        n+=1\n",
    "        \n",
    "        # Check scenario \n",
    "        success = check_scenario(df_f, df_u, seed_sp)\n",
    "\n",
    "        # Add success \n",
    "        nb_success['n'] = n\n",
    "        for m in metrics_to_test:\n",
    "            nb_success[m] += success[m]\n",
    "            print(m, ' ', nb_success[m]/n)\n",
    "            \n",
    "    # Save dict\n",
    "    with open('success.pkl', 'wb') as fp:\n",
    "        pickle.dump(nb_success, fp)\n",
    "        print('dictionary saved successfully to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for att_1 in range_atts[0]:\n",
    "    for att_2 in range_atts[1]:\n",
    "        cond = (a.att_1 == att_1) & (a.att_2 == att_2)\n",
    "        print(att_1, ' ', att_2, ' ', 1-mean_squared_error(a[cond].label, a[cond].pred), 1-mean_squared_error(b[cond].label, b[cond].pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('success.pkl', 'rb') as fp:\n",
    "    b = pickle.load(fp)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
